{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from os import path\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from json import dump, load\n",
    "from textdistance import levenshtein\n",
    "import pyLDAvis.gensim\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "ru_stopwords = stopwords.words('russian')\n",
    "alpha_tokenizer = RegexpTokenizer('[A-Za-zА-Яа-я]\\w+')\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join('..', 'data', 'citcon4bundles.txt'), 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_groups = defaultdict(lambda: {})\n",
    "errors = []\n",
    "\n",
    "for line in lines:\n",
    "    try:\n",
    "        context_group, text = line.split(' ', 1)\n",
    "        splits = text.split(' ', 3)\n",
    "        citation_text = [morph.parse(word.lower())[0].normal_form for word in alpha_tokenizer.tokenize(splits[3]) if word not in ru_stopwords]\n",
    "        if len(citation_text) < 3:\n",
    "            errors.append(line)\n",
    "        citation_code = '_'.join(splits[:3])\n",
    "        context_groups[context_group][citation_code] = citation_text\n",
    "    except ValueError:\n",
    "        errors.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_topics(topics):\n",
    "    topics_list = []\n",
    "    pretty_output = ''\n",
    "    pretty_topics = [', '.join([re.findall('\"([^\"]*)\"', s)[0] for s in topic[1].split(' + ')]) for topic in topics]\n",
    "    for i, topic in enumerate(pretty_topics):\n",
    "        pretty_output += 'Topic {}: {}; '.format(i, topic)\n",
    "        topics_list.append(topic)\n",
    "    return pretty_output, topics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics_by_ids(ids, topic_list, ref_key, topics_counts):\n",
    "    pretty_output = []\n",
    "    probs = []\n",
    "    for topic, prob in ids:\n",
    "        probs.append(round(prob, 2))\n",
    "        pretty_output.append({'ref_key': ref_key, 'topic': topic_list[topic], 'probability': round(prob, 2)})\n",
    "        topics_counts[topic_list[topic]].append(round(prob, 2))\n",
    "    return pretty_output, probs, topics_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {}\n",
    "topics_dist = defaultdict(lambda: {})\n",
    "word_counts = defaultdict(lambda: 0)\n",
    "\n",
    "for key, citation in context_groups.items():\n",
    "    try:\n",
    "        dictionary = Dictionary(citation.values())\n",
    "        bow_corpus = [dictionary.doc2bow(doc) for doc in citation.values()]\n",
    "        lda_model = LdaMulticore(bow_corpus, num_topics=3, id2word=dictionary, passes=2, workers=2)\n",
    "        topics[key], topics_list = pretty_print_topics(lda_model.print_topics(num_topics=3, num_words=5))\n",
    "        topics_d = []\n",
    "        probs = []\n",
    "        topics_counts = defaultdict(lambda: [])\n",
    "        for topic in topics_list:\n",
    "            topic_words = topic.split(', ')\n",
    "            for word in topic_words:\n",
    "                word_counts[word] += 1\n",
    "        s = 0\n",
    "        for i in range(len(bow_corpus)):\n",
    "            pretty_output, probs_, topics_counts = print_topics_by_ids(lda_model[bow_corpus[i]], topics_list, list(citation.keys())[i], topics_counts)\n",
    "            s += len(pretty_output)\n",
    "            topics_d.extend(pretty_output)   \n",
    "            probs.extend(probs_)\n",
    "        topics_counts_ = []\n",
    "        for key_, value_ in topics_counts.items():\n",
    "            temp_dict = {}\n",
    "            temp_dict['topic'] = key_\n",
    "            temp_dict['number'] = len(value_)\n",
    "            temp_dict['probability_average'] = round(np.average(value_), 3)\n",
    "            temp_dict['probability_std'] = round(np.std(value_), 3)\n",
    "            topics_counts_.append(temp_dict)\n",
    "        topics_dist[key]['topics'] = sorted(topics_counts_, key=lambda k: k['number'], reverse=True) \n",
    "        topics_dist[key]['contexts'] = sorted(topics_d, key=lambda k: k['probability'], reverse=True) \n",
    "#         visdata = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "#         pyLDAvis.save_html(visdata, path.join('..', 'data', 'new_vis', '{}_vis.html'.format(key)))\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "topics_dist = dict(topics_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('topic_output.json', 'w') as f:\n",
    "    dump(dict(topics_dist), f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
