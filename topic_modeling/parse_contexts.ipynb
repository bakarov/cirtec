{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from os import path\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from json import dump\n",
    "from textdistance import levenshtein\n",
    "\n",
    "ru_stopwords = stopwords.words('russian')\n",
    "alpha_tokenizer = RegexpTokenizer('[A-Za-zА-Яа-я]\\w+')\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join('..', 'data', 'citcon4bundles.txt'), 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_groups = defaultdict(lambda: {})\n",
    "errors = []\n",
    "\n",
    "for line in lines:\n",
    "    try:\n",
    "        context_group, text = line.split(' ', 1)\n",
    "        splits = text.split(' ', 3)\n",
    "        citation_text = [morph.parse(word.lower())[0].normal_form for word in alpha_tokenizer.tokenize(splits[3]) if word not in ru_stopwords]\n",
    "        citation_code = '_'.join(splits[:3])\n",
    "        context_groups[context_group][citation_code] = citation_text\n",
    "    except ValueError:\n",
    "        errors.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_topics(topics):\n",
    "    topics_list = []\n",
    "    pretty_output = ''\n",
    "    pretty_topics = [', '.join([re.findall('\"([^\"]*)\"', s)[0] for s in topic[1].split(' + ')]) for topic in topics]\n",
    "    for i, topic in enumerate(pretty_topics):\n",
    "        pretty_output += 'Topic {}: {}; '.format(i, topic)\n",
    "        topics_list.append(topic)\n",
    "    return pretty_output, topics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics_by_ids(old_lda_topics, ids, topic_list, ref_key):\n",
    "    pretty_output = []\n",
    "#     current_topic = ', '.join(topic_list)\n",
    "#     dist = levenshtein.distance(current_topic, old_lda_topics[ref_key.split('_')[0]])\n",
    "    for topic, prob in ids:\n",
    "        pretty_output.append({'ref_key': ref_key, 'topic': topic_list[topic], 'probability': str(round(prob, 2))})\n",
    "    return pretty_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join('..', 'data', 'topics_lda_dist.txt'), 'r') as f:\n",
    "    lda_dist = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_lda_topics = {}\n",
    "\n",
    "for line in lda_dist[:~0]:\n",
    "    key, text = line.split(' ', 1)\n",
    "    topics = text.split(', probability')[0]\n",
    "    if key in lda_topics.keys():\n",
    "        old_lda_topics[key.split('citing:')[1]] = '{}, {}'.format(lda_topics[key], topics)\n",
    "    else:\n",
    "        old_lda_topics[key.split('citing:')[1]] = topics\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {}\n",
    "topics_dist = defaultdict(lambda: [])\n",
    "\n",
    "for key, citation in context_groups.items():\n",
    "    try:\n",
    "        dictionary = Dictionary(citation.values())\n",
    "        bow_corpus = [dictionary.doc2bow(doc) for doc in citation.values()]\n",
    "        lda_model = LdaMulticore(bow_corpus, num_topics=3, id2word=dictionary, passes=2, workers=2)\n",
    "        topics[key], topics_list = pretty_print_topics(lda_model.print_topics(num_topics=3, num_words=5))\n",
    "        for i in range(len(bow_corpus)):\n",
    "            topics_dist[key].append(print_topics_by_ids(old_lda_topics, lda_model[bow_corpus[i]], topics_list, list(citation.keys())[i]))\n",
    "    except ValueError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('topic_output.json', 'w') as f:\n",
    "    dump(dict(topics_dist), f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
